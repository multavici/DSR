{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Recommender\n",
    "\n",
    "## Intro\n",
    "\n",
    "The purpose of this exercise is to use Spark in a real dataset, instead of just a toy example.\n",
    "\n",
    "You will use the data from the [Yelp Dataset Challenge](https://www.yelp.de/dataset_challenge), which contains information about businesses, users, reviews and more.\n",
    "\n",
    "For this exercise, you will need to focus only on the following files:\n",
    "- yelp_academic_dataset_business.json\n",
    "- yelp_academic_dataset_review.json\n",
    "\n",
    "The goal is to build a recommender using [Spark's ALS (Alternating Least Squares)](https://spark.apache.org/docs/2.3.0/ml-collaborative-filtering.html) and then generate recommendations for a given user.\n",
    "\n",
    "Since the dataset is quite big, you should pick a business category (e.g. Restaurants) and a city (e.g. Edinburgh) and work on the recommender using only this subset of the data.\n",
    "\n",
    "Please take some time to:\n",
    "- find out what information you will need to feed as input to Spark's ALS\n",
    "- check how this information is available in the dataset\n",
    "- plan how you will tackle this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small version of the Yelp dataset\n",
    "#!wget https://s3.us-west-2.amazonaws.com/dsr-spark-appliedml/yelp_dataset_small.tar.gz\n",
    "#!tar -xvzf yelp_dataset_small.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SQLContext\n",
    "#sc = SparkContext('local[*]')\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Data\n",
    "\n",
    "- Load the file ***yelp_academic_dataset_business.json*** and select the following columns:\n",
    "    - business_id\n",
    "    - name\n",
    "    - city\n",
    "    - stars\n",
    "    - categories\n",
    "    - address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = sqlc.read.json('yelp_academic_dataset_business.json')\n",
    "df_business = df_business.select('business_id',\n",
    "                                 'name',\n",
    "                                 'city',\n",
    "                                 'stars',\n",
    "                                 'categories',\n",
    "                                 'address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='0DI8Dt2PJp07XkVvIElIcQ', name='Innovative Vapors', city='Tempe', stars=4.5, categories=['Tobacco Shops', 'Nightlife', 'Vape Shops', 'Shopping'], address='227 E Baseline Rd, Ste J2')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_business.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a business category\n",
    "\n",
    "- Define a regular Python function that takes a list of categories and returns 1 if a category of your choice (for instance, 'Restaurants') is contained in the list of categories or 0 otherwise\n",
    "- Using the Python function, define a Spark's User Defined Function (UDF) with an IntegerType return\n",
    "- Using the UDF, filter the businesses that belong to the category you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def is_category_listed(name, categories):\n",
    "    listed = 0\n",
    "    if categories is not None:\n",
    "        if name in categories:\n",
    "            listed = 1\n",
    "    return listed\n",
    "\n",
    "def is_restaurant(categories):\n",
    "    return is_category_listed('Restaurants', categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_is_restaurant = UserDefinedFunction(is_restaurant, \n",
    "                                        IntegerType())\n",
    "\n",
    "df_restaurants = df_business.withColumn('is_restaurant', udf_is_restaurant('categories')) \\\n",
    "                            .filter('is_restaurant == 1') \\\n",
    "                            .drop('is_restaurant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='EDqCEAGXVGCH4FJXgqtjqg', name='Pizza Pizza', city='Toronto', stars=2.5, categories=['Restaurants', 'Pizza', 'Chicken Wings', 'Italian'], address='979 Bloor Street W')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_restaurants.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The UDF approach works just fine, but there is a more straightforward way to perform the same operation\n",
    "    - hint: look at ***array_contains*** SQL function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# you can overwrite the former df_restaurants\n",
    "df_restaurants = (df_business\n",
    "                  .filter(F.array_contains(F.col('categories'),\n",
    "                                           'Restaurants')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a city\n",
    "- Having filtered by the business category, now it is time to filter by the city (for instance, Edinburgh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_restaurants = df_restaurants.filter('city = \"Edinburgh\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating numeric IDs\n",
    "- If you haven't done it yet, take one sample from your already filtered DataFrame and notice that the ***business_id*** contains an alphanumeric value - this is not good for Spark's ALS implementation, which requires IDs for items (in our case, businesses) and users to be numeric\n",
    "- Use a ***StringIndexer*** to create a new column ***business_idn*** from the conversion of business_id into a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "business_indexer = StringIndexer().setInputCol('business_id') \\\n",
    "                                  .setOutputCol('business_idn')\n",
    "    \n",
    "business_idx_model = business_indexer.fit(df_city_restaurants)\n",
    "df_city_restaurants = business_idx_model.transform(df_city_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='NsarUMMMPOlMBb6K04x6hw', name='Juice Almighty', city='Edinburgh', stars=4.5, categories=['Food', 'Fast Food', 'Restaurants', 'Juice Bars & Smoothies'], address='7A Castle Street, Corstorphine', business_idn=24.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_restaurants.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, name: string, city: string, stars: double, categories: array<string>, address: string, business_idn: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_restaurants.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Data\n",
    "\n",
    "- Load the file ***yelp_academic_dataset_review.json*** and select the following columns:\n",
    "    - user_id\n",
    "    - business-id\n",
    "    - stars\n",
    "    - date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = sqlc.read.json('yelp_academic_dataset_review.json')\n",
    "df_reviews = df_reviews.select('user_id',\n",
    "                               'business_id',\n",
    "                               'stars',\n",
    "                               'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping reviews for the chosen city only\n",
    "\n",
    "- You are only interested in reviews of businesses you kept after filtering for category and city - how to filter out everything else? (hint: take a look at the ***join*** operation of DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_reviews = (df_reviews\n",
    "                   .join(df_city_restaurants\n",
    "                         .select('business_id', 'business_idn'),\n",
    "                         on='business_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating numeric IDs\n",
    "\n",
    "- As it happened with the ***business_id***, you also need to convert ***user_id*** into a numeric value - once again, use a ***StringIndexer*** to create a new column named ***user_idn*** containing the result of the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indexer = StringIndexer().setInputCol('user_id') \\\n",
    "                              .setOutputCol('user_idn').setHandleInvalid('keep')\n",
    "\n",
    "user_idx_model = user_indexer.fit(df_city_reviews)\n",
    "df_city_reviews = user_idx_model.transform(df_city_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, user_id: string, stars: bigint, date: string, business_idn: double, user_idn: double]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_reviews.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a sequential number to the user's reviews\n",
    "\n",
    "- Now add a ***sequential number*** to the user's reviews, that is, for each user, order his/her reviews by date (multiple reviews on the same date can be randomly ordered) and number them (hint: check ***window functions***)\n",
    "- This sequential number will be useful later to perform a time-wise split of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window().partitionBy('user_idn').orderBy('date')\n",
    "df_city_reviews = (df_city_reviews\n",
    "                   .withColumn('review_seq', F.row_number().over(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting reviews to keep only users with more than 4 reviews\n",
    "\n",
    "- Some users had rated only 1 or a few businesses - this would pose as a problem to make recommendations - so you would want to keep only users who had rated more than 4 reviews, for instance\n",
    "- Find the ***total number of reviews*** for each user and then filter them using this information (hint: again, you can use a ***window function***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().partitionBy('user_idn')\n",
    "df_city_reviews = df_city_reviews.withColumn('n_reviews', F.max('review_seq').over(w))\n",
    "df_selected = df_city_reviews.filter('n_reviews > 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, user_id: string, stars: bigint, date: string, business_idn: double, user_idn: double, review_seq: int, n_reviews: int]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating mean rating by user\n",
    "\n",
    "- Now you can calculate the mean rating by user and make it into a dictionary where the key is the ***user_id*** (hint: look at ***rdd*** method of DataFrames and ***collectAsMap*** method of RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_user_means = df_selected.select('user_id', 'stars') \\\n",
    "                             .groupby('user_id') \\\n",
    "                             .mean() \\\n",
    "                             .rdd \\\n",
    "                             .collectAsMap()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering rating by user\n",
    "\n",
    "- The dictionary containing mean ratings by user can be seen as a ***lookup table*** - what is the appropriate way of dealing with those in Spark?\n",
    "- Once you have figured this out, define a regular Python function that takes two arguments - ***user_id*** (String) and ***rating*** (String, which you will need to convert to float inside the function) - and returns the result of subtracting the mean rating of the user from the rating parameter\n",
    "- Using the Python function, define a Spark's User Defined Function (UDF) with a DoubleType return\n",
    "- Using the UDF, create a column in your DataFrame with the centered ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "lookup_user_means = sc.broadcast(dict_user_means)\n",
    "\n",
    "def zero_mean(user_id, rating):\n",
    "    return (float(rating) - lookup_user_means.value.get(user_id))\n",
    "\n",
    "udf_zero_mean = UserDefinedFunction(zero_mean , DoubleType())\n",
    "\n",
    "df_centered = df_selected.withColumn('centered', udf_zero_mean('user_id','stars')) \\\n",
    "                .drop('stars') \\\n",
    "                .withColumnRenamed('centered', 'stars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once again, the UDF approach is not the most \"Sparkonic\" way of handling this - can you perform the same operation using only functions from ***pyspark.sql.functions*** (which was imported earlier as F)?\n",
    "    - hint: you'll need ***Window functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can overwrite df_centered\n",
    "w = Window.partitionBy('user_id')\n",
    "df_centered = (df_selected\n",
    "               .withColumn('avg_stars', F.avg('stars').over(w))\n",
    "               .withColumn('stars', F.expr('stars - avg_stars'))\n",
    "               .drop('avg_stars'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "df_selected = df_selected.withColumn('stars', F.col('stars').astype('float'))\n",
    "\n",
    "@pandas_udf(df_selected.schema, PandasUDFType.GROUPED_MAP)\n",
    "# Input/output are both a pandas.DataFrame\n",
    "def pandas_subtract_mean(pdf):\n",
    "    return pdf.assign(stars=pdf.stars - pdf.stars.mean())\n",
    "\n",
    "df_centered = df_selected.groupby('user_id').apply(pandas_subtract_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training and test sets by time\n",
    "\n",
    "- In recommender systems, it is common practice to do the training/test split timewise, that is, the test set is composed of the latest reviews\n",
    "- First, filter only those reviews which have a sequential number smaller than the ***total number of reviews***, by user: this is your training set\n",
    "- Then, filter only those reviews which have a sequential number identical to the ***total number of reviews***, by user: this is your test set\n",
    "- Now you can see why you had to add a sequential number to the user's reiews - since some users had done all his/her reviews on the same day, you need to disambiguate them to split the dataset. By doing this, you guarantee your test set will have only 1 review for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df_centered.filter('review_seq < n_reviews')\n",
    "df_test = df_centered.filter('review_seq = n_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If using Spark 2.1 (as in the Docker image), you need to filter out \"new\" businesses in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#businesses = df_training.select('business_id').distinct()\n",
    "#df_test = df_test.join(businesses, on='business_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Least Squares (ALS) Model\n",
    "\n",
    "- This is the recommender itself - the ALS uses a iterative approach to find the underlying factors that yield the user/item rating matrix\n",
    "- It takes as input a DataFrame with three columns, representing:\n",
    "    - userCol: user IDs (numeric - remember the conversion you did)\n",
    "    - itemCol: item IDs (numeric - remember the conversion you did)\n",
    "    - ratingCol: rating (numeric, obviously)\n",
    "    - coldStartStrategy: \"drop\" (if there is unseen data on the test set, meaning a new user/business, drop it) - ***only available from Spark 2.2 on***\n",
    "- Its parameters are:\n",
    "    - rank: the number of factors to consider\n",
    "    - maxIter: the maximum number of iterations to perform\n",
    "    - regParam: the regularization parameter\n",
    "- Use Spark's ALS to fit a model based on your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "params = {'rank': 10, \n",
    "          'maxIter': 10, \n",
    "          'regParam': 0.3}\n",
    "\n",
    "als = ALS(userCol=\"user_idn\",\n",
    "          itemCol=\"business_idn\", \n",
    "          ratingCol=\"stars\", \n",
    "          coldStartStrategy=\"drop\",\n",
    "          seed=42).setParams(**params)\n",
    "\n",
    "model = als.fit(df_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for the training set\n",
    "\n",
    "- Once the model is trained, make predictions for the training set and use a ***RegressionEvaluator*** to find out the RMSE of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7114699825165288\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(df_training)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                labelCol=\"stars\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "train_rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(train_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for the test set\n",
    "\n",
    "- Now, make predictions for the test set and use a ***RegressionEvaluator*** to find out the RMSE of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9260549386555614\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(df_test)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                labelCol=\"stars\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "test_rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "Now, your model is trained, but how can you use it to make recommendations for a given user?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing business data\n",
    "\n",
    "- It would not make sense to recommend a place the user has already rated, right? So, generate a dictionary where ***user_idn*** is the key and a list of the already rated ***business_idn*** is the value (hint: when aggregating DataFrames, ***collect_list*** is a VERY useful function to turn multiple records into a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "dict_visited_by_user = df_city_reviews.select('user_idn', 'business_idn') \\\n",
    "                                      .groupby('user_idn') \\\n",
    "                                      .agg(collect_list('business_idn')) \\\n",
    "                                      .rdd \\\n",
    "                                      .collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Besides, recommending a given business_id also does not help much, right? So you need to organize the business data in a way it can be shown to the user.\n",
    "    - Define a regular Python function that takes one argument ***row*** (Row type) and returns a dictionary where ***business_idn*** is the key and the value is yet another dictionary with relevant fields (for instance: name, address, stars, categories)\n",
    "    - Transform your business DataFrame into an RDD and apply the function you defined - upon collecting, you will end up with a list of dictionaries\n",
    "    - Transform this list of dictionaries into a single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rest_to_json(row):\n",
    "    return {row.business_idn: {'name': row.name, \n",
    "                               'address': row.address,\n",
    "                               'stars': row.stars, \n",
    "                               'categories': row.categories}}\n",
    "\n",
    "rest = df_city_restaurants.select('business_idn',\n",
    "                                  'name',\n",
    "                                  'address',\n",
    "                                  'stars',\n",
    "                                  'categories') \\\n",
    "                          .rdd \\\n",
    "                          .map(rest_to_json) \\\n",
    "                          .collect()\n",
    "\n",
    "dict_rest = {k: v for d in rest for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making recommendations for a user\n",
    "\n",
    "- To actually make the recommendations, we need to build an input DataFrame to feed the model\n",
    "    - A DataFrame can be created using the SQL Context and a list of Rows, each containg two columns: user_idn and business_idn - the rating will be computed by the model\n",
    "    - But you only need to have rows for the businesses which were not yet rated by the user - from all businesses, exclude the ones already rated by him/her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "user_idn = 317\n",
    "n_business = len(dict_rest)\n",
    "\n",
    "visited = dict_visited_by_user[user_idn]\n",
    "not_visited = list(set(range(n_business)).difference(set(visited)))\n",
    "\n",
    "rows_user = [Row(user_idn=user_idn, \n",
    "                 business_idn=float(i)) for i in not_visited]\n",
    "\n",
    "df_test_user = sqlc.createDataFrame(rows_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, you can use the generated DataFrame to make predictions\n",
    "    - If there are any NA predictions, make sure to turn them into a really bad value (for instance, -5.0) (hint: remember ***na*** method of DataFrames)\n",
    "- Order the predictions and take the ***business_idn*** of the top 5\n",
    "- Finally, use this information to fetch the business data from the dictionary you assembled a couple of steps ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df_test_user).na.fill(-5.0)\n",
    "\n",
    "top_predictions = predictions.orderBy(desc('prediction')) \\\n",
    "                             .select('business_idn') \\\n",
    "                             .rdd \\\n",
    "                             .map(lambda row: row.business_idn) \\\n",
    "                             .take(5)\n",
    "\n",
    "response = list(map(lambda idn: dict_rest[idn], top_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'address': '112 St Stephen St',\n",
       "  'categories': ['Restaurants', 'Chinese'],\n",
       "  'name': \"Karen's Unicorn\",\n",
       "  'stars': 5.0},\n",
       " {'address': 'Ogilivie Terrace, Harrison Park',\n",
       "  'categories': ['Food', 'Cafes', 'Restaurants'],\n",
       "  'name': 'Zazou Cruises',\n",
       "  'stars': 4.5},\n",
       " {'address': '36 Broughton Street',\n",
       "  'categories': ['Nightlife', 'Restaurants', 'British', 'Bars'],\n",
       "  'name': 'Seasons Restaurant & Bar',\n",
       "  'stars': 5.0},\n",
       " {'address': '54 Shore, Leith',\n",
       "  'categories': ['French', 'Restaurants', 'British'],\n",
       "  'name': 'Martin Wishart',\n",
       "  'stars': 5.0},\n",
       " {'address': '125 Comiston Road, Morningside',\n",
       "  'categories': ['Coffee & Tea',\n",
       "   'Restaurants',\n",
       "   'Patisserie/Cake Shop',\n",
       "   'Cafes',\n",
       "   'Food'],\n",
       "  'name': 'Marie DÃ©lices',\n",
       "  'stars': 4.5}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idn = 317\n",
    "rows_user = [Row(user_idn=user_idn)]\n",
    "df_test_user2 = sqlc.createDataFrame(rows_user)\n",
    "recommendations = model.recommendForUserSubset(df_test_user2, 5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_idn=301, rating=1.018609881401062),\n",
       " Row(business_idn=1239, rating=0.9607487320899963),\n",
       " Row(business_idn=1395, rating=0.9601149559020996),\n",
       " Row(business_idn=573, rating=0.9247788786888123),\n",
       " Row(business_idn=4, rating=0.91269850730896)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations.recommendations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06317136436700821,\n",
       " 0.0064953905530273914,\n",
       " 0.32015344500541687,\n",
       " 0.2631630599498749,\n",
       " 0.48920193314552307,\n",
       " 0.3880455195903778,\n",
       " 0.07208795845508575,\n",
       " 0.5377174019813538,\n",
       " 0.48443400859832764,\n",
       " 0.41213491559028625]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors = model.userFactors.filter('id == 317').toPandas()\n",
    "factors.features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you finished the exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
