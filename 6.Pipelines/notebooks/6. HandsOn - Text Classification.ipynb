{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a data set containing customers reviews on musical instruments. In this notebook we will work on text data to create a classification of the users rating (source: http://jmcauley.ucsd.edu/data/amazon/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('../data/Musical_Instruments_5.json',lines=True)\n",
    "# for the moment we will only take reviewText and overall\n",
    "df = data[['reviewText','overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>02 28, 2014</td>\n",
       "      <td>A2IBPI20UZIR0U</td>\n",
       "      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n",
       "      <td>good</td>\n",
       "      <td>1393545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>5</td>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>03 16, 2013</td>\n",
       "      <td>A14VAT5EAX3D9S</td>\n",
       "      <td>Jake</td>\n",
       "      <td>Jake</td>\n",
       "      <td>1363392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>08 28, 2013</td>\n",
       "      <td>A195EZSQDW3E21</td>\n",
       "      <td>Rick Bennette \"Rick Bennette\"</td>\n",
       "      <td>It Does The Job Well</td>\n",
       "      <td>1377648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>02 14, 2014</td>\n",
       "      <td>A2C00NNG1ZQQG2</td>\n",
       "      <td>RustyBill \"Sunday Rocker\"</td>\n",
       "      <td>GOOD WINDSCREEN FOR THE MONEY</td>\n",
       "      <td>1392336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>02 21, 2014</td>\n",
       "      <td>A94QU4C90B1AX</td>\n",
       "      <td>SEAN MASLANKA</td>\n",
       "      <td>No more pops when I record my vocals.</td>\n",
       "      <td>1392940800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   helpful  overall  \\\n",
       "0  1384719342    [0, 0]        5   \n",
       "1  1384719342  [13, 14]        5   \n",
       "2  1384719342    [1, 1]        5   \n",
       "3  1384719342    [0, 0]        5   \n",
       "4  1384719342    [0, 0]        5   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Not much to write about here, but it does exac...  02 28, 2014   \n",
       "1  The product does exactly as it should and is q...  03 16, 2013   \n",
       "2  The primary job of this device is to block the...  08 28, 2013   \n",
       "3  Nice windscreen protects my MXL mic and preven...  02 14, 2014   \n",
       "4  This pop filter is great. It looks and perform...  02 21, 2014   \n",
       "\n",
       "       reviewerID                                      reviewerName  \\\n",
       "0  A2IBPI20UZIR0U  cassandra tu \"Yeah, well, that's just like, u...   \n",
       "1  A14VAT5EAX3D9S                                              Jake   \n",
       "2  A195EZSQDW3E21                     Rick Bennette \"Rick Bennette\"   \n",
       "3  A2C00NNG1ZQQG2                         RustyBill \"Sunday Rocker\"   \n",
       "4   A94QU4C90B1AX                                     SEAN MASLANKA   \n",
       "\n",
       "                                 summary  unixReviewTime  \n",
       "0                                   good      1393545600  \n",
       "1                                   Jake      1363392000  \n",
       "2                   It Does The Job Well      1377648000  \n",
       "3          GOOD WINDSCREEN FOR THE MONEY      1392336000  \n",
       "4  No more pops when I record my vocals.      1392940800  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  Not much to write about here, but it does exac...        5\n",
       "1  The product does exactly as it should and is q...        5\n",
       "2  The primary job of this device is to block the...        5\n",
       "3  Nice windscreen protects my MXL mic and preven...        5\n",
       "4  This pop filter is great. It looks and perform...        5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "- Calculate the percentage of data belonging to each rating class (from 1 to 5).\n",
    "- Which problems and difficulties could we face in the predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>size</th>\n",
       "      <th>perc_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>217</td>\n",
       "      <td>0.021148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "      <td>0.024364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>772</td>\n",
       "      <td>0.075236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2084</td>\n",
       "      <td>0.203099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6938</td>\n",
       "      <td>0.676152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  size  perc_total\n",
       "0        1   217    0.021148\n",
       "1        2   250    0.024364\n",
       "2        3   772    0.075236\n",
       "3        4  2084    0.203099\n",
       "4        5  6938    0.676152"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg = df.groupby(['overall']).size().reset_index(name='size')\n",
    "df_agg['perc_total'] = df_agg['size']/df_agg['size'].sum()\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an imbalanced set, way more 4 and 5 star reviews than 1 and 2 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a couple of examples of bad reviews (overall=1 or 2) and of good reviews (overall=5 or 4) to have a feeling how users write..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It hums, crackles, and I think I'm having problems with my equipment.  As soon as I use any of my other cords then the problem is gone.  Hosa makes some other products that have good value.  But based on my experience I don't recommend this one.\n",
      "I'm a pro-cheapo and I hated this thing. They're noisy, and the cables feel really cheap, gummy-like. Drop few more bucks and get something else!\n",
      "Received it in time, standard blister packaging but the cable stopped working after 45 days. Since I was out of 30 days return time, sent an email to the manufacturer but no response till today.\n",
      "These things are terrible. One wouldn't fit in my soundboard, another wouldn't lock in my mic. Plain and simple....if your looking for crap that doesn't work....buy this.\n",
      "This is a cheap piece of junk that does what it says, it distorts, You want something to make your guitar sound like junk, this will do it. Why pedals that make your guitar sound like a piece of junk are the most popular is beyond me. This is not a boutique Distortion pedal like Hendrix would use. Boutique and distortion are an oxymoron.These things are built with heavy metal, it should last, I kinda hope it doesn't(just kidding), you want distortion, you got distortion\n",
      "I bought this to use with my keyboard. I wasn't really aware that there were other options for keyboard pedals. It doesn't work as smoothly as the pedals do on an acoustic piano, which is what I'd always used. Doesn't have the same feel either. Nowhere close.In my opinion, a sustain pedal like the M-Audio SP-2 Sustain Pedal with Piano Style Action or other similar pedal is a much better choice. The price difference is only a few dollars and the feel and action are so much better. The only thing I've found the Yamaha FC-5 Sustain Pedal is good for is taking up space in a drawer where I keep my electric guitar pedals.\n",
      "I didn't expect this cable to be so thin. It's easily 1/2 the thickness of any guitar cable I've used. Not sure about long-term durability or signal loss/interference. If I had the foresight I'd spend a couple extra bucks on a thicker cable.  Still, it works and was inexpensive.EDIT: 6 months later and it's dead already.  Wire frayed at the right angle jack.  You get what you pay for.\n",
      "I bought this for my Canon Vixia HF G10 Video Camera and a Shotgun Mic that I recently purchased for it. I needed a 12&#34; wire for it to be a perfect fit from my mic (which is mounted on the Mini Advanced Shoe) to the microphone plug on the Video Camera...The wire on this thing is *not* 1 foot long. the actual wire is under 8&#34; long itself. I would have returned this thing but it was cheap enough that it wasn't worth the hassle... I'd have rated it 1 star, but I tried it on my Nikon D3200 (Shoe mounted Shotgun Mike) and it worked out ok. Still a tight fit though (since there's only 8&#34; of wire)...I'd recommend anyone looking for a short cable go with a 2' adapter; worst case scenario, you can zip-tie a loop instead of getting angry that it doesn't fit.Also, not sure if it's the wire (so this didn't affect stars) but I hear a hiss when recording. Again; unsure if it's the way the wires are stretched out to the max on the DSLR or not (or my shotgun mic).\n",
      "I got 3 of these to plug XLR cables into the 1/4&#34; jacks on my board. I got it because my board only has 6 channels with XLR inputs. None of them work. Perhaps I got the wrong thing?\n",
      "The handle and spring strength make this uncomfortable to place on the guitar, and I feel rushed to clamp on the capo because it's unpleasant to use - hurts the fingers. It's cheap and easy to use. It'll do the job, but I don't like it.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(df[df['overall'] == 1]['reviewText'].iloc[i])\n",
    "    \n",
    "for i in range(5):\n",
    "    print(df[df['overall'] == 2]['reviewText'].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, y_train, y_test = train_test_split(df, df['overall'], test_size=0.1, random_state=0)\n",
    "\n",
    "X = df.reviewText.values\n",
    "y = df.overall.values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, \n",
    "                                          y,\n",
    "                                          test_size=0.1,\n",
    "                                          random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>So good that I bought another one.  Love the h...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I have used monster cables for years, and with...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I now use this cable to run from the output of...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Perfect for my Epiphone Sheraton II.  Monster ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Monster makes the best cables and a lifetime w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Monster makes a wide array of cables, includin...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I got it to have it if I needed it. I have fou...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>If you are not use to using a large sustaining...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I love it, I used this for my Yamaha ypt-230 a...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I bought this to use in my home studio to cont...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I bought this to use with my keyboard. I wasn'...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>This Fender cable is the perfect length for me...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>wanted it just on looks alone...It is a nice l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I've been using these cables for more than 4 m...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Fender cords look great and work just as well....</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>This is a cool looking cheap cable which works...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Fender 18 Feet California Clear Instrument...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Very good cable. Well made and it looks great ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Got this cable to run a rockband keyboard cont...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>When I was searching for MIDI cables for my AR...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Cant go wrong. Great quality on a budget price...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The ends of the midi cable look and feel like ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Just trying to find a midi to midi was a task,...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The Hosa XLR cables are affordable and very he...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I bought these to go from my board to the amp....</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10231</th>\n",
       "      <td>For a long time, I never thought much about gu...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10232</th>\n",
       "      <td>My father is a full-time gigging musician prim...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10233</th>\n",
       "      <td>D'Addario has always been one of the best stri...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10234</th>\n",
       "      <td>I usually use Gibson Vintage Nickle Strings on...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10235</th>\n",
       "      <td>Excellent tone, and I'm a bit surprised by tha...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10236</th>\n",
       "      <td>I've been stringing my guitars with D'Addario ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10237</th>\n",
       "      <td>Don't get me wrong, the improvement over the o...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10238</th>\n",
       "      <td>These have a nice bright sound and are easy on...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10239</th>\n",
       "      <td>D'Addario's NYXL1046 Nickel Plated Electric Gu...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10240</th>\n",
       "      <td>Just put these on my Martin DCX1E and they sou...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10241</th>\n",
       "      <td>I ordered this for my husband, the musician. H...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10242</th>\n",
       "      <td>I had used Elixer strings for several years on...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10243</th>\n",
       "      <td>I can't speak on the claim for a longer string...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10244</th>\n",
       "      <td>Great product with nice feel and tone. I'm onl...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10245</th>\n",
       "      <td>I have reviewed these strings before, and fran...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10246</th>\n",
       "      <td>I had the opportunity to try a set of Elixir N...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10247</th>\n",
       "      <td>I was looking forward to trying these, as I've...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10248</th>\n",
       "      <td>I put these strings on my Guild acoustic elect...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10249</th>\n",
       "      <td>I ordered this for my husband, the musician.  ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10250</th>\n",
       "      <td>I had used Elixer strings for several years on...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10251</th>\n",
       "      <td>True to phosphor bronze these strings have a m...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10252</th>\n",
       "      <td>I've used Elixirs for about five years now. Th...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10253</th>\n",
       "      <td>I'm a D'Addario man myself, but hey free is fr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10254</th>\n",
       "      <td>I really like these strings.  While they are n...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10255</th>\n",
       "      <td>I have lots of friends who play these strings....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10256</th>\n",
       "      <td>Great, just as expected.  Thank to all.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10257</th>\n",
       "      <td>I've been thinking about trying the Nanoweb st...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10258</th>\n",
       "      <td>I have tried coated strings in the past ( incl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10259</th>\n",
       "      <td>Well, MADE by Elixir and DEVELOPED with Taylor...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10260</th>\n",
       "      <td>These strings are really quite good, but I wou...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10261 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              reviewText  overall\n",
       "0      Not much to write about here, but it does exac...        5\n",
       "1      The product does exactly as it should and is q...        5\n",
       "2      The primary job of this device is to block the...        5\n",
       "3      Nice windscreen protects my MXL mic and preven...        5\n",
       "4      This pop filter is great. It looks and perform...        5\n",
       "5      So good that I bought another one.  Love the h...        5\n",
       "6      I have used monster cables for years, and with...        5\n",
       "7      I now use this cable to run from the output of...        3\n",
       "8      Perfect for my Epiphone Sheraton II.  Monster ...        5\n",
       "9      Monster makes the best cables and a lifetime w...        5\n",
       "10     Monster makes a wide array of cables, includin...        5\n",
       "11     I got it to have it if I needed it. I have fou...        4\n",
       "12     If you are not use to using a large sustaining...        3\n",
       "13     I love it, I used this for my Yamaha ypt-230 a...        5\n",
       "14     I bought this to use in my home studio to cont...        5\n",
       "15     I bought this to use with my keyboard. I wasn'...        2\n",
       "16     This Fender cable is the perfect length for me...        4\n",
       "17     wanted it just on looks alone...It is a nice l...        5\n",
       "18     I've been using these cables for more than 4 m...        5\n",
       "19     Fender cords look great and work just as well....        5\n",
       "20     This is a cool looking cheap cable which works...        4\n",
       "21     The Fender 18 Feet California Clear Instrument...        5\n",
       "22     Very good cable. Well made and it looks great ...        4\n",
       "23     Got this cable to run a rockband keyboard cont...        5\n",
       "24     When I was searching for MIDI cables for my AR...        5\n",
       "25     Cant go wrong. Great quality on a budget price...        4\n",
       "26     The ends of the midi cable look and feel like ...        5\n",
       "27     Just trying to find a midi to midi was a task,...        5\n",
       "28     The Hosa XLR cables are affordable and very he...        4\n",
       "29     I bought these to go from my board to the amp....        5\n",
       "...                                                  ...      ...\n",
       "10231  For a long time, I never thought much about gu...        5\n",
       "10232  My father is a full-time gigging musician prim...        5\n",
       "10233  D'Addario has always been one of the best stri...        5\n",
       "10234  I usually use Gibson Vintage Nickle Strings on...        5\n",
       "10235  Excellent tone, and I'm a bit surprised by tha...        5\n",
       "10236  I've been stringing my guitars with D'Addario ...        5\n",
       "10237  Don't get me wrong, the improvement over the o...        3\n",
       "10238  These have a nice bright sound and are easy on...        5\n",
       "10239  D'Addario's NYXL1046 Nickel Plated Electric Gu...        5\n",
       "10240  Just put these on my Martin DCX1E and they sou...        5\n",
       "10241  I ordered this for my husband, the musician. H...        5\n",
       "10242  I had used Elixer strings for several years on...        5\n",
       "10243  I can't speak on the claim for a longer string...        5\n",
       "10244  Great product with nice feel and tone. I'm onl...        5\n",
       "10245  I have reviewed these strings before, and fran...        4\n",
       "10246  I had the opportunity to try a set of Elixir N...        5\n",
       "10247  I was looking forward to trying these, as I've...        2\n",
       "10248  I put these strings on my Guild acoustic elect...        5\n",
       "10249  I ordered this for my husband, the musician.  ...        5\n",
       "10250  I had used Elixer strings for several years on...        5\n",
       "10251  True to phosphor bronze these strings have a m...        5\n",
       "10252  I've used Elixirs for about five years now. Th...        5\n",
       "10253  I'm a D'Addario man myself, but hey free is fr...        4\n",
       "10254  I really like these strings.  While they are n...        5\n",
       "10255  I have lots of friends who play these strings....        3\n",
       "10256            Great, just as expected.  Thank to all.        5\n",
       "10257  I've been thinking about trying the Nanoweb st...        5\n",
       "10258  I have tried coated strings in the past ( incl...        4\n",
       "10259  Well, MADE by Elixir and DEVELOPED with Taylor...        4\n",
       "10260  These strings are really quite good, but I wou...        4\n",
       "\n",
       "[10261 rows x 2 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr shape (9234,)\n",
      "X_te shape (1027,)\n",
      "y_tr shape (9234,)\n",
      "y_te shape (1027,)\n"
     ]
    }
   ],
   "source": [
    "print('X_tr shape',np.shape(X_tr))\n",
    "print('X_te shape',np.shape(X_te))\n",
    "print('y_tr shape',np.shape(y_tr))\n",
    "print('y_te shape',np.shape(y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extract numerical features from the text so that we can use them for classification.\n",
    "One way to achieve this is to use `sklearn.feature_extraction.text.CountVectorizer`: this scikit-learn class find tokens (words) in the text and create a matrix of occurrences counts. Another possibility is to use `sklearn.feature_extraction.text.TfidfVectorizer`: this method returns *weights* of words instead of the absolute count based on the appearence of each word in the all corpus of documents. The idea is that word that appears often in a corpus are not very significant to make predictions and should be given a lower weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words\n",
    "--------------------\n",
    "\n",
    "Different types of vectorizers:\n",
    "\n",
    "<ul>\n",
    "<li>```sklearn.feature_extraction.text.CountVectorizer``` - Counts the number of times a word appears in the text</li>\n",
    "<li>```sklearn.feature_extraction.text.TfidfVectorizer``` - Weighs the words according to the importance of the word in the context of whole collection. Is the word ```the``` important if it appears in all documents?</li>\n",
    "<li>```sklearn.feature_extraction.text.HashingVectorizer``` - Useful when you don't know the vocabulary upfront. Feature number is calculated as ```hash(token) % vocabulary_size```.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "https://github.com/logicai-io/pipelines-sklearn\n",
    "1. Use ```CountVectorizer``` / ```TfidfVectorizer``` to fit the collection of documents\n",
    "2. How many unique tokens are there in text? Print some examples (ie first few hundred).\n",
    "3. What methods you can use to reduce this number? \n",
    "   - Check out and experiment with the arguments: ```ngram_range```, ```min_df```. How the vocabulary size changes with each change?\n",
    "   - Would you make use of stop_words? Check ```CountVectorizer``` documentation\n",
    "   - What would you replace / delete from the text?\n",
    "4. Write a custom function `clean_text` that accepts a text as input and transforms it (remove/hash numbers, delete short/long words etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), min_df=0.1, max_df=0.7, stop_words='english') \n",
    "# ngram_range lets you set the amount of words considered per token\n",
    "# min_df lets you set the minimum occurences of a word to be considered as a token\n",
    "tfidvectorizer = TfidfVectorizer()\n",
    "X_ = vectorizer.fit_transform(X)\n",
    "X__ = tfidvectorizer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total amount of unique tokens: 27\n",
      "['better', 'bought', 'buy', 'does', 'don', 'easy', 'good', 'great', 'guitar', 'just', 'like', 'little', 'need', 'nice', 'pedal', 'price', 'quality', 'really', 'sound', 'strings', 'time', 'use', 'used', 'using', 've', 'work', 'works']\n"
     ]
    }
   ],
   "source": [
    "print(f'total amount of unique tokens: {len(vectorizer.get_feature_names())}')\n",
    "print(vectorizer.get_feature_names()[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total amount of unique tokens: 20551\n",
      "['00', '000', '000s', '001', '003', '004', '0045', '007', '008', '008ex', '009', '009s', '00e', '00ibanez', '00mm', '01', '010', '010s', '011', '0115', '011s', '012', '0125', '012dlx', '012s', '013', '0135', '013s', '016', '017', '01s', '02', '020', '022uf', '024w', '025', '026', '03', '032', '032w', '034', '036', '04', '042', '042w', '046i', '04mm', '05', '050', '0502']\n"
     ]
    }
   ],
   "source": [
    "print(f'total amount of unique tokens: {len(tfidvectorizer.get_feature_names())}')\n",
    "print(tfidvectorizer.get_feature_names()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape\n",
    "X_.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10261, 20551)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X__.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    raw_words = text.split(\" \")\n",
    "    words = []\n",
    "    for raw_word in raw_words:\n",
    "        b = False\n",
    "        if len(raw_word) < 4:\n",
    "            break\n",
    "        if len(raw_word) > 14:\n",
    "            break\n",
    "        if raw_word.isdigit():\n",
    "            break\n",
    "        for char in raw_word:\n",
    "            if char.isdigit():\n",
    "                b = True\n",
    "                break\n",
    "        if b == True: break  \n",
    "        words.append(raw_word.lower())\n",
    "    return ' '.join(word for word in words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(preprocessor=clean_text, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total amount of unique tokens: 1501\n",
      "['able', 'ableton', 'abrams', 'absolutely', 'accessary', 'accessories', 'accessory', 'according', 'accurate', 'acessorie', 'acoustic', 'acoustics', 'acquired', 'action', 'actually', 'adapter', 'adapters', 'adario', 'addario', 'added', 'addition', 'adds', 'adjustable', 'adjustments', 'admittedly', 'aesome', 'affordable', 'ago', 'ahem', 'akorg', 'alchemy', 'allotoday', 'allow', 'allows', 'aloha', 'alot', 'alright', 'alto', 'aluminum', 'amazing', 'amazingly', 'amazon', 'amp', 'amplified', 'amplifier', 'amps', 'analog', 'angle', 'annoying', 'apparently']\n"
     ]
    }
   ],
   "source": [
    "vectorizer.fit(X)\n",
    "print(f'total amount of unique tokens: {len(vectorizer.get_feature_names())}')\n",
    "print(vectorizer.get_feature_names()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "<div>\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    return t\n",
    "\n",
    "\n",
    "vectorizers = [\n",
    "     (\"simple\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "    ,(\"preprocessing + min_df=10 + stop_words\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10,\n",
    "                          stop_words=stop_words.ENGLISH_STOP_WORDS))\n",
    "]\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n",
    "    print('and' in vect.get_feature_names())\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple\n",
      "['00', '000', '000s', '001', '003', '004', '0045', '007', '008', '008ex']\n",
      "19596\n",
      "True\n",
      "preprocessing\n",
      "['aa', 'aaa', 'aaaand', 'ab', 'aback', 'abalone', 'abandon', 'abbe', 'abehringer', 'abelton']\n",
      "18180\n",
      "True\n",
      "preprocessing + min_df=10\n",
      "['aa', 'aaa', 'ability', 'able', 'ableton', 'about', 'above', 'absolute', 'absolutely', 'abuse']\n",
      "3831\n",
      "True\n",
      "preprocessing + min_df=10 + stop_words\n",
      "['aa', 'aaa', 'ability', 'able', 'ableton', 'absolute', 'absolutely', 'abuse', 'abused', 'ac']\n",
      "3566\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "import re\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    return t\n",
    "\n",
    "\n",
    "vectorizers = [\n",
    "     (\"simple\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "    ,(\"preprocessing + min_df=10 + stop_words\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10,\n",
    "                          stop_words=stop_words.ENGLISH_STOP_WORDS))\n",
    "]\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n",
    "    print('and' in vect.get_feature_names())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization\n",
    "------------------\n",
    "\n",
    "Stemming and Lemmatization are two linguistic normalization techniques that consists on grouping together words that derivate from the same origing and have the same meaning:\n",
    "\n",
    "    connection\n",
    "    connections\n",
    "    connective     --->   connect\n",
    "    connected\n",
    "    connecting\n",
    "\n",
    "**Stemming** works by reducing a group of words to the same *stem* based on the origin of the words in the group. The stem does not necessarly have to be a word in the language. With **lemmatization**, on the other hand, makes sure that the root word is a proper word (*lemma*) in the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/falkvandermeirsch/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/falkvandermeirsch/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('universally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'universally'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('universally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'universally'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer.lemmatize('universally',pos=wordnet.ADV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('is',pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "\n",
    "Extend the function `clean_text` created above to a function that, after cleaning the text, applies a stemming technique to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    raw_words = text.split(\" \")\n",
    "    words = []\n",
    "    for raw_word in raw_words:\n",
    "        b = False\n",
    "        if len(raw_word) < 4:\n",
    "            break\n",
    "        if len(raw_word) > 14:\n",
    "            break\n",
    "        if raw_word.isdigit():\n",
    "            break\n",
    "        for char in raw_word:\n",
    "            if char.isdigit():\n",
    "                b = True\n",
    "                break\n",
    "        if b == True: break \n",
    "        raw_word = lemmatizer.lemmatize(raw_word)\n",
    "        words.append(raw_word.lower())\n",
    "    return ' '.join(word for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total amount of unique tokens: 1457\n",
      "['able', 'ableton', 'abrams', 'absolutely', 'accessary', 'accessories', 'accessory', 'according', 'accurate', 'acessorie', 'acoustic', 'acquired', 'action', 'actually', 'adapter', 'adario', 'add', 'addario', 'added', 'addition', 'adds', 'adjustable', 'adjustment', 'adjustments', 'admittedly', 'aesome', 'affordable', 'ago', 'ahem', 'akorg', 'alchemy', 'allotoday', 'allow', 'allows', 'aloha', 'alot', 'alright', 'alto', 'aluminum', 'amazing', 'amazingly', 'amazon', 'amp', 'amplified', 'amplifier', 'analog', 'angle', 'annoying', 'apparently', 'appeared']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(preprocessor=clean_text, stop_words='english')\n",
    "vectorizer.fit(X)\n",
    "print(f'total amount of unique tokens: {len(vectorizer.get_feature_names())}')\n",
    "print(vectorizer.get_feature_names()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions**\n",
    "<div>\n",
    "def clean_text_stem(text):\n",
    "    t = clean_text(text)\n",
    "    tokens = nltk.word_tokenize(t)    # alternative: [w for w in t.split()]\n",
    "    stemmed_tokens = [stemmer.stem(w) for w in tokens] \n",
    "    stemmed_text = ' '.join(stemmed_tokens)\n",
    "    return stemmed_text\n",
    "    \n",
    "vectorizers = [\n",
    "     (\"simple\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "    ,(\"preprocessing + min_df=10 + stop_words\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10,\n",
    "                          stop_words=stop_words.ENGLISH_STOP_WORDS))\n",
    "    ,(\"stemming + min_df=10 + stop_words\",\n",
    "          CountVectorizer(preprocessor=clean_text_stem,\n",
    "                          min_df=10,\n",
    "                          stop_words=stop_words.ENGLISH_STOP_WORDS))\n",
    "]\n",
    "\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n",
    "    print('and' in vect.get_feature_names())\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "----------------------\n",
    "\n",
    "Now that we know how to transform text data, let's put it into a pipeline.\n",
    "1. Create a pipeline with `CountVectorizer`, `StandardScaler` and a classifier (ex `RandomForestClassifier`)\n",
    "2. Using ```sklearn.metrics.classification_report``` create a report about your classifier\n",
    "3. Play with text preprocessing in ```CountVectorizer``` to see if the model improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.14      0.02      0.03       202\n",
      "           2       0.00      0.00      0.00       225\n",
      "           3       0.14      0.02      0.03       696\n",
      "           4       0.27      0.05      0.09      1862\n",
      "           5       0.68      0.95      0.80      6249\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      9234\n",
      "   macro avg       0.25      0.21      0.19      9234\n",
      "weighted avg       0.53      0.66      0.56      9234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    8.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(preprocessor=clean_text, stop_words='english')),\n",
    "                ('scaling', StandardScaler(with_mean=False)),\n",
    "                ('clf', RandomForestClassifier())])\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_tr, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9234x1366 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6928 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(preprocessor=clean_text, stop_words='english')\n",
    "cv.fit(X_tr)\n",
    "X_tr_trans = cv.transform(X_tr)\n",
    "X_tr_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I ordered two of these stands and they work very well! Definitely keeps my guitars a lot safer than having them leaning against things.',\n",
       "       'Works well, pretty straight forward. A little over priced for what it is, but, sure beats having to purchase another complete stand.',\n",
       "       'Installed on two guitars and I love them! I can switch out many different guitar straps without worrying that they will fall off. Great product, would recommend to any guitar player!',\n",
       "       ...,\n",
       "       'It was a great item. very nice strings. It looked really good on my acoustic and electric guitar. Works great',\n",
       "       'i like the fact they are colored the helps a set up and break downi am going to buy some more',\n",
       "       \"This original plug fits the purpose perfectly! Quality and precision! The plugs don't take up space (couldn't be smaller) and the wiring is pliable and easy to rout in and out of spaces on your pedalboard. Keep away from the cheap imitations that rob you of space and are hard to work with! Make sure what you receive is a One Spot product in its original packaging (check images).\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.102014294996751"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_tr, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       202\n",
      "           2       0.40      0.01      0.02       225\n",
      "           3       0.13      0.01      0.02       696\n",
      "           4       0.32      0.10      0.15      1862\n",
      "           5       0.69      0.95      0.80      6249\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      9234\n",
      "   macro avg       0.31      0.21      0.20      9234\n",
      "weighted avg       0.55      0.67      0.58      9234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    6.8s finished\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([('vect', CountVectorizer()),\n",
    "                ('scaling', StandardScaler(with_mean=False)),\n",
    "                ('clf', RandomForestClassifier())])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "<div>\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "                ('scaling', StandardScaler(with_mean=False)),\n",
    "                ('clf', RandomForestClassifier())])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds))\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "----------------------\n",
    "\n",
    "1. Use `GridSearchCV` or `RandomizedSearchCV` to find the best parameters for the models. Check at least 2 parameters.\n",
    "\n",
    "**GridSearchCV** - you specify the exact values of the parameters you want to test\n",
    "**RandomizedSearchCV** - you specify ranges of parameters\n",
    "\n",
    "\n",
    "2. Inspect the attribute `cv_results_` after fitting. It gives a nice representation of the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search\n",
      "\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:   43.6s finished\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-1e287d8013fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mgrid_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-1e287d8013fb>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mgrid_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'str'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"Grid search\")\n",
    "print()\n",
    "\n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'vect__binary': [True, False]}\n",
    "             \n",
    "grid_clf = GridSearchCV(clf, params, scoring='f1_weighted', n_jobs=1, verbose=True)\n",
    "grid_clf.fit(X_tr, y_tr)\n",
    "\n",
    "best_params = sorted(grid_clf.cv_results_, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params) \n",
    "#    \n",
    "#print(\"Randomized search\")\n",
    "#print()\n",
    "#    \n",
    "#params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "#          'vect__analyzer': [\"word\",\"char\"],\n",
    "#          'model__lr__dimensions': [100, 200]}\n",
    "#\n",
    "#grid_clf = RandomizedSearchCV(clf, params, n_jobs=1, verbose=True, n_iter=8)\n",
    "#grid_clf.fit(np.array(X_tr[:10000]), y_tr[:10000])\n",
    "#\n",
    "#best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "#\n",
    "#for params, score, _ in best_params:\n",
    "#    print(score, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions**\n",
    "<div>\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"Grid search\")\n",
    "print()\n",
    "\n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'vect__binary': [True, False]}\n",
    "             \n",
    "grid_clf = GridSearchCV(clf, params, scoring='f1_weighted', n_jobs=1, verbose=True)\n",
    "grid_clf.fit(X_tr, y_tr)\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params) \n",
    "#    \n",
    "#print(\"Randomized search\")\n",
    "#print()\n",
    "#    \n",
    "#params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "#          'vect__analyzer': [\"word\",\"char\"],\n",
    "#          'model__lr__dimensions': [100, 200]}\n",
    "#\n",
    "#grid_clf = RandomizedSearchCV(clf, params, n_jobs=1, verbose=True, n_iter=8)\n",
    "#grid_clf.fit(np.array(X_tr[:10000]), y_tr[:10000])\n",
    "#\n",
    "#best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "#\n",
    "#for params, score, _ in best_params:\n",
    "#    print(score, params)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract further information from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text review contains a number of information that can be used for classification.\n",
    "Examples:\n",
    "- length of text\n",
    "- punctuation\n",
    "- emoji usage \n",
    "\n",
    "Example:\n",
    "\"The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "\n",
    "1. Create a custom Transformer that extract the length of the review \n",
    "2. Create a custom Transformer that detect number of positive emoticons \n",
    "3. Add the transformers above to the features extraction part in the pipeline above. **ATTENTION**: Make sure the transform methods return a sparse matrix (use `from scipy import sparse` `sparse.csr_matrix`), otherwise you will get dimensions error from numpy concatenation method used in FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractLength():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X\n",
    "        for index, row in X_.iterrows():\n",
    "            row['reviewLength'] = len(row['reviewText'])\n",
    "        return X_\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reviewLength'] = len(data['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>02 28, 2014</td>\n",
       "      <td>A2IBPI20UZIR0U</td>\n",
       "      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n",
       "      <td>good</td>\n",
       "      <td>1393545600</td>\n",
       "      <td>10261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>5</td>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>03 16, 2013</td>\n",
       "      <td>A14VAT5EAX3D9S</td>\n",
       "      <td>Jake</td>\n",
       "      <td>Jake</td>\n",
       "      <td>1363392000</td>\n",
       "      <td>10261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>08 28, 2013</td>\n",
       "      <td>A195EZSQDW3E21</td>\n",
       "      <td>Rick Bennette \"Rick Bennette\"</td>\n",
       "      <td>It Does The Job Well</td>\n",
       "      <td>1377648000</td>\n",
       "      <td>10261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>02 14, 2014</td>\n",
       "      <td>A2C00NNG1ZQQG2</td>\n",
       "      <td>RustyBill \"Sunday Rocker\"</td>\n",
       "      <td>GOOD WINDSCREEN FOR THE MONEY</td>\n",
       "      <td>1392336000</td>\n",
       "      <td>10261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>02 21, 2014</td>\n",
       "      <td>A94QU4C90B1AX</td>\n",
       "      <td>SEAN MASLANKA</td>\n",
       "      <td>No more pops when I record my vocals.</td>\n",
       "      <td>1392940800</td>\n",
       "      <td>10261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   helpful  overall  \\\n",
       "0  1384719342    [0, 0]        5   \n",
       "1  1384719342  [13, 14]        5   \n",
       "2  1384719342    [1, 1]        5   \n",
       "3  1384719342    [0, 0]        5   \n",
       "4  1384719342    [0, 0]        5   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Not much to write about here, but it does exac...  02 28, 2014   \n",
       "1  The product does exactly as it should and is q...  03 16, 2013   \n",
       "2  The primary job of this device is to block the...  08 28, 2013   \n",
       "3  Nice windscreen protects my MXL mic and preven...  02 14, 2014   \n",
       "4  This pop filter is great. It looks and perform...  02 21, 2014   \n",
       "\n",
       "       reviewerID                                      reviewerName  \\\n",
       "0  A2IBPI20UZIR0U  cassandra tu \"Yeah, well, that's just like, u...   \n",
       "1  A14VAT5EAX3D9S                                              Jake   \n",
       "2  A195EZSQDW3E21                     Rick Bennette \"Rick Bennette\"   \n",
       "3  A2C00NNG1ZQQG2                         RustyBill \"Sunday Rocker\"   \n",
       "4   A94QU4C90B1AX                                     SEAN MASLANKA   \n",
       "\n",
       "                                 summary  unixReviewTime  reviewLength  \n",
       "0                                   good      1393545600         10261  \n",
       "1                                   Jake      1363392000         10261  \n",
       "2                   It Does The Job Well      1377648000         10261  \n",
       "3          GOOD WINDSCREEN FOR THE MONEY      1392336000         10261  \n",
       "4  No more pops when I record my vocals.      1392940800         10261  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_length = ExtractLength()\n",
    "X_ = extract_length.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'reviewLength'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/DSR/env/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'reviewLength'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-6abc456b37b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviewLength'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/DSR/env/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DSR/env/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DSR/env/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DSR/env/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DSR/env/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'reviewLength'"
     ]
    }
   ],
   "source": [
    "X_['reviewLength'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Click here to see the solution**\n",
    "<div>\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import sparse\n",
    "\n",
    "class LengthExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, extracts road name column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        l = np.array([len(x) for x in X])\n",
    "        return sparse.csr_matrix(l).T\n",
    "    \n",
    "\n",
    "class CountEmojis(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, extracts road name column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        l_pos = np.array([len(re.findall('(:\\)|:D|;\\)|:\\])', x)) for x in X])\n",
    "        return sparse.csr_matrix(l_pos).T\n",
    "        #return l_pos\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "        ('len', LengthExtractor())\n",
    "    ])),\n",
    "    ('scaling', StandardScaler(with_mean=False)),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to improve it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
